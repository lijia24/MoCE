#bash scripts/run_train_debug.sh configs/hmdb51/hmdb_train.yaml
#bash scripts/run_train_debug_2.sh configs/hmdb51/hmdb_train.yaml
#bash scripts/run_train.sh configs/hmdb51/hmdb_train.yaml
resume:
pretrain:
seed: 1024
data:
    dataset: hmdb51
    modality: video
    num_segments: 8
    seg_length: 1       # no use
    batch_size: 256
    workers: 8
    num_classes: 26
    image_tmpl: 'img_{:05d}.jpg' # no used
    train_root: 'xxx/hmdb51'
    train_list: 'lists/hmdb51/train_all2.txt' 
    val_root: 'xxx/hmdb51'
    val_list: 'lists/hmdb51/val_froster.txt'
    label_list: 'lists/hmdb51/hmdb51_rephrase_merged.csv'
    spatial_label_list: 'lists/hmdb51/hmdb51_spatial_label.csv'
    template_label_list: 'lists/hmdb51/hmdb51_temporal_label.csv'
    input_size: 224
    random_shift: True
    output_path: exps_MoTE
network:
    arch: ViT-B/16      #ViT-B/32 ViT-B/16
    init: True
    tm: False           # no use
    drop_out: 0.0 
    emb_dropout: 0.0
    sim_header: Transf  # [Transf, None] 'Transf'ï¼š6-layer temporal transformer  'None': mean temporal pooling
    interaction: DP     # [DP] 'DP': mean temporal pooling
    joint_st: False     # whether use joint space-time attention in the transformer (default: False)
    drop: 0      
    fix_text: True
    fix_video: False
    num_experts: 4      #  >1: MoTE; <=1: mlp
solver:
    type: cosine
    epochs: 30
    start_epoch: 0
    epoch_offset: 0
    optim: adamw
    lr: 5.e-5
    lr_warmup_step: 5
    weight_decay: 0.2
    loss_type: CE
    evaluate: False     # only run evaluation
    clip_ratio: 0.07
    grad_accumulation_steps: 1
logging:
    print_freq: 10
    eval_freq: 1